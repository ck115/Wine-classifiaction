Information that will be helpful with working with the modified data set

New data: The train and test data was seperated using stratified train_test_split, with a test percentage of 0.2 of the entire dataset.

    data/red_wine_train.csv
     
    data/red_wine_test.csv



Overall features are not that great, we don't have super higher correlations with quality but these are the ones that most likely will be helpful

    Label = quality

    Important Features: The higher the positve correlation, or the lower the negative correlation the more effect it has on our dependent variable(quality).
                        the ones with underscores or manufactured ones which are explained in notes section (possibly overfitting features)

        alcohol (0.48)

        alcohol_higher (0.4)

        volatile acidity(-.39)

        va_high (-.3)

        sulphates (0.25)

        citric acid (0.23) -- has a strong correlation with volatile acidity so might want to be careful for overfitting 

        total sulfur dioxide (-.19)

        density (-0.17) -- has a very strong correlation with alcohol so might want to leave out for overfitting



    Maybe Features: These are features that will most likely confuse models, and there is a high chance that we should just leave them out. 
                    I didn't remove them because if one of us wanted to test them, we can.

        fixed acidity (.12)

        chlorides (-.13)
        





Notes:

    Scaling: 
        For the most part the scaling for the data will be MinMax or Standard, and choosing depends on the model you are using. Therefore,
        all the data in here is the raw data. If you scale, make sure to scale both the training and testing dataset.

    Pre-processing:
    
        Missing values: 
            No missing values whatsoever

        Outliers: 
            There are quite a few outliers in this dataset, and determining whether to keep or not keep is mostly up to determining the value
            of the outliers. This is not easy and sometimes not necessary, so I will leave it up to individual implementations to do so.

    Accuracy Measure: In all models, we will have to use an accuracy measure that works with imbalanced datasets

    Bin-able features: 
        Alcohol and volatile acidity where the only features that seemed that binning was really and option.
        Most likely Neural Network implementation does not require this, but for decision trees and KNN, this can help generalization.
        If you decide to bin, make sure the test set is also changed as well.    

    Possible overfitting features: 
        The two features with underscores are helper features that correlate to swtichpoints in the two repsective features
        they mention. The Goal is that they can help make decision regarding the strongest features, and are simiplier version
        that can be used in KNN to reduce time to predict. However including them with the others might lead to overfitting, so
        be carefule and just try with and without them to see what works best.



